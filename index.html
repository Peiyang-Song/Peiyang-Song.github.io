<!DOCTYPE HTML>
<html lang="en">

<head>    
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-0B5LHBSZYY"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-0B5LHBSZYY');
  </script>

  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Peiyang Song</title>

  <meta name="author" content="Peiyang Song">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="apple-touch-icon" sizes="180x180" href="images/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="images/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="images/favicon-16x16.png">
  <link rel="manifest" href="images/site.webmanifest">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <table style="width:100%;max-width:1050px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:1.5%;width:70%;vertical-align:middle">
              <p style="text-align:center">
                <name>Peiyang Song</name>
              </p>
              <p>
		I am an undergraduate student studying Computer Science at California Institute of Technology (Caltech), advised by Prof. <a style="text-decoration: none" target="_blank" href="https://netlab.caltech.edu/">Steven Low</a>, with a minor in Robotics advised by Prof. <a style="text-decoration: none" target="_blank" href="https://scholar.google.com/citations?user=-YP8MJ0AAAAJ&hl=en">Günter Niemeyer</a>. I am a researcher in the <a style="text-decoration: none" target="_blank" href="https://ai.stanford.edu/">Stanford AI Lab (SAIL)</a>, advised by Prof. <a style="text-decoration: none" target="_blank" href="https://cocolab.stanford.edu/ndg.html">Noah Goodman</a> in the <a style="text-decoration: none" target="_blank" href="https://cocolab.stanford.edu/">Computation & Cognition Lab (CoCoLab)</a>. I have also been fortunate to work with Prof. <a style="text-decoration: none" target="_blank" href="http://tensorlab.cms.caltech.edu/users/anima/">Anima Anandkumar</a> (Caltech), Dr. <a style="text-decoration: none" target="_blank" href="https://yangky11.github.io/">Kaiyu Yang</a> (Meta), Prof. <a style="text-decoration: none" target="_blank" href="https://www.arch.cs.ucsb.edu/prof-sherwood">Tim Sherwood</a> (UC Santa Barbara), and Dr. <a style="text-decoration: none" target="_blank" href="https://dl.acm.org/profile/81100206077">Jeremy Lau</a> (Google) during my undergrad.
              </p>
              <p style="text-align:center">
                宋沛洋 &nbsp/&nbsp
                <a target="_blank" href="mailto:psong@caltech.edu">Email</a> &nbsp/&nbsp
                <a target="_blank" href="data/Peiyang_Song_CV.pdf">CV</a> &nbsp/&nbsp
                <a target="_blank" href="data/Bio.txt">Bio</a> &nbsp/&nbsp
                <a target="_blank" href="https://scholar.google.com/citations?user=E1j11NQAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a target="_blank" href="https://github.com/Peiyang-Song">GitHub</a> &nbsp/&nbsp
		<a target="_blank" href="https://www.linkedin.com/in/peiyang-song-3279b3251/">LinkedIn</a> &nbsp/&nbsp
		<a target="_blank" href="https://twitter.com/p_song1">Twitter</a>
              </p>
            </td>
            <td style="padding:2.5%;width:33%;max-width:33%">
              <a href="images/Peiyang Song.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/Peiyang Song - Circle.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
	<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
	      <p><b>[Sep. 2024]</b>  Our paper <a href="https://arxiv.org/abs/2410.00988">Creative and Context-Aware Translation of East Asian Idioms with GPT-4</a> is accepted to EMNLP 2024 Findings.<br>
		 <b>[Sep. 2024]</b>  Our paper on <a href="https://arxiv.org/abs/2409.15454">LLM inhibitory control & A-not-B cognitive errors</a> is accepted to EMNLP 2024 Findings.<br>
		 <b>[Sep. 2024]</b>  I am giving an invited tutorial at <a href="https://neurosymbolic.github.io/nsss2024/">NSSS 2024</a> on Neuro-Symbolic Theorem Proving with Lean: <a href="data/Neuro-Symbolic Theorem Proving with Lean.pdf">slides</a>.<br>
<!-- 		 <b>[Aug. 2024]</b>  Our paper on <a href="https://openreview.net/pdf?id=qu6sMJmEwl">trustworthy LLM reasoning in ICL</a> is accepted to ICML 2024 Workshop on LLMs and Cognition.<br> -->
		 <b>[June 2024]</b>  I am joining <a href="https://ai.stanford.edu/">Stanford AI Lab (SAIL)</a> and <a href="https://cocolab.stanford.edu/">CoCoLab</a>, working on mathematical reasoning with LLMs.<br>
		 <b>[May  2024]</b>  Attending <a href="https://www.neusconference.org/">NeuS</a> at Berkeley, CA, discussing Neuro-Symbolic AI for Math and Science.<br>
<!-- 		 <b>[Feb. 2024]</b>  Our paper <a href="https://dl.acm.org/doi/10.1145/3620665.3640395">Energy Efficient Convolutions with Temporal Arithmetic</a> is accepted to ASPLOS 2024.<br> -->
<!-- 		 <b>[Dec. 2023]</b>  Attending <a href="https://nips.cc/">NeurIPS</a> at New Orleans, LA, presenting <a href="https://leandojo.org">LeanDojo</a> and <a href="https://https://github.com/lean-dojo/LeanCopilot">Lean Copilot</a>.<br> -->
<!-- 		 <b>[Nov. 2023]</b>  Our paper <a href="https://https://github.com/lean-dojo/LeanCopilot">Lean Copilot</a> is accepted to NeurIPS 2023 MATH-AI Workshop.<br> -->
<!-- 		 <b>[Sep. 2023]</b>  Our paper <a href="https://leandojo.org">LeanDojo</a> is accepted to NeurIPS 2023 Datasets and Benchmarks Track as an Oral Presentation.<br> -->
<!-- 		 <b>[Sep. 2023]</b>  We released <a target="_blank" href="https://github.com/lean-dojo/LeanCopilot">Lean Copilot</a> for LLMs to act as copilots for theorem proving in Lean: <a target="_blank" href="https://youtu.be/OxFcZU5ihBk">Video demo</a>.<br> -->
<!-- 		 <b>[Aug. 2023]</b>  Honored to receive the prestigious <a target="_blank" href="https://ersp.cs.ucsb.edu/home">Early Research Scholarship</a>!<br> -->
<!-- 	         <b>[June 2023]</b>  We released <a href="https://leandojo.org">LeanDojo</a>, an open-source playground for LLMs to prove formal theorems in Lean.<br> -->
<!-- 	         <b>[May  2023]</b>  I am officially joining Caltech <a target="_blank" href="http://tensorlab.cms.caltech.edu/users/anima/">Anima AI + Science Lab</a>, working on neuro-symbolic reasoning and more!<br> -->
<!--             <b>[Apr. 2023]</b>  My research received the prestigious <a target="_blank" href="https://sfp.caltech.edu/undergraduate-research/programs/surf">Caltech SFP SURF Award</a>!<br> -->
<!-- 	         <b>[Feb. 2023]</b>  I am officially joining <a target="_blank" href="https://www.arch.cs.ucsb.edu/">UCSB ArchLab</a>, working on energy-efficient machine learning and more!</p> -->
            </td>
          </tr>
		
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
	      <p>My current research interest is mainly in machine reasoning, especially AI for mathematics and code generation. In the past, I also worked on energy-efficient machine learning systems and machine translation.</p>
<!-- 		      Representative papers are <span class="highlight">highlighted</span>. -->
            </td>
          </tr>
		
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

	  <tr onmouseout="idiom_stop()" onmouseover="idiom_start()">
            <td style="padding:5px;width:17%;vertical-align:middle">
              <div class="one">
                <div class="two" id='idiom_image'>
                  <img src='images/idiom-1.png' width="150">
                </div>
                <img src='images/idiom-2.png' width="150">
              </div>
              <script type="text/javascript">
                function idiom_start() {
                  document.getElementById('idiom_image').style.opacity = "1";
                }

                function idiom_stop() {
                  document.getElementById('idiom_image').style.opacity = "0";
                }
                idiom_stop()
              </script>
            </td>
            <td style="padding:20px;width:83%;vertical-align:middle">
              <a target="_blank" href="https://arxiv.org/abs/2410.00988">
                <papertitle>Creative and Context-Aware Translation of East Asian Idioms with GPT-4</papertitle>
              </a>
              <br />
	      <a target="_blank" href="https://www.linkedin.com/in/kenan-tang-1682a11a0/">Kenan Tang</a>*, <strong>Peiyang Song</strong>*, <a target="_blank" href="https://yaoqin1.github.io/">Yao Qin</a>, and <a target="_blank" href="https://sites.cs.ucsb.edu/~xyan/">Xifeng Yan</a> (* <strong>Equal Contribution</strong>)
	      <br />
              <em>Findings of Empirical Methods in Natural Language Processing (EMNLP)</em>, 2024
              <br />
	      <a target="_blank" href="https://arxiv.org/abs/2410.00988">arXiv</a>
              /
	      <a target="_blank" href="https://kenantang.github.io/cjk-idioms-gpt/">code</a>
              <p></p>
              <p>
                To compile a dictionary of East Asian idiom translations demands much time and creativity even for expert translators. To alleviate such burden, we automate such high-quality data generation with GPT-4, and identify Pareto-optimal prompting strategies on both faithfulness and creativity, outperforming translation engines and human baseline.
              </p>
            </td>
          </tr>

	  <tr onmouseout="a_not_b_stop()" onmouseover="a_not_b_start()">
            <td style="padding:5px;width:17%;vertical-align:middle">
              <div class="one">
                <div class="two" id='a_not_b_image'>
                  <img src='images/a_not_b-1.png' width="150">
                </div>
                <img src='images/a_not_b-2.png' width="150">
              </div>
              <script type="text/javascript">
                function a_not_b_start() {
                  document.getElementById('a_not_b_image').style.opacity = "1";
                }

                function a_not_b_stop() {
                  document.getElementById('a_not_b_image').style.opacity = "0";
                }
                a_not_b_stop()
              </script>
            </td>
            <td style="padding:20px;width:83%;vertical-align:middle">
              <a target="_blank" href="https://arxiv.org/abs/2409.15454">
                <papertitle>In-Context Learning May Not Elicit Trustworthy Reasoning: A-Not-B Errors in Pretrained Language Models</papertitle>
              </a>
              <br />
	      <a target="_blank" href="https://pengrui-han.github.io/">Pengrui Han</a>*, <strong>Peiyang Song</strong>*, <a target="_blank" href="https://haofeiyu.me/">Haofei Yu</a>, and <a target="_blank" href="https://cs.stanford.edu/~jiaxuan/">Jiaxuan You</a> (* <strong>Equal Contribution</strong>)
	      <br />
              <em>Findings of Empirical Methods in Natural Language Processing (EMNLP)</em>, 2024
              <br />
	      <a target="_blank" href="https://arxiv.org/abs/2409.15454">arXiv</a>
              /
	      <a target="_blank" href="https://github.com/Peiyang-Song/LLM-A-Not-B-Errors">code</a>
              <p></p>
              <p>
                Motivated by the crucial cognitive phenomenon of A-not-B errors, we present the first systematic evaluation on the surprisingly vulnerable inhibitory control abilities of LLMs. We reveal that this weakness undermines LLMs' trustworthy reasoning capabilities across diverse domains, and introduce various mitigations.
              </p>
            </td>
          </tr>

	  <tr onmouseout="lean_copilot_stop()" onmouseover="lean_copilot_start()">
            <td style="padding:5px;width:17%;vertical-align:middle">
              <div class="one">
                <div class="two" id='lean_copilot_image'>
                  <img src='images/lean_copilot-1.png' width="150">
                </div>
                <img src='images/lean_copilot-2.png' width="150">
              </div>
              <script type="text/javascript">
                function lean_copilot_start() {
                  document.getElementById('lean_copilot_image').style.opacity = "1";
                }

                function lean_copilot_stop() {
                  document.getElementById('lean_copilot_image').style.opacity = "0";
                }
                lean_copilot_stop()
              </script>
            </td>
            <td style="padding:20px;width:83%;vertical-align:middle">
              <a target="_blank" href="https://github.com/lean-dojo/LeanCopilot">
                <papertitle>Towards Large Language Models as Copilots for Theorem Proving in Lean</papertitle>
              </a>
              <br />
	      <strong>Peiyang Song</strong>, <a target="_blank" href="https://yangky11.github.io/">Kaiyu Yang</a>, and <a target="_blank" href="http://tensorlab.cms.caltech.edu/users/anima/">Anima Anandkumar</a>
	      <br />
              <em>NeurIPS Mathematical Reasoning and AI (MATH-AI) Workshop</em>, 2023
              <br />
	      <a target="_blank" href="https://arxiv.org/abs/2404.12534">arXiv</a>
	      /
              <a target="_blank" href="https://github.com/lean-dojo/LeanCopilot">code</a>
              /
              <a target="_blank" href="data/lean_copilot_poster.pdf">poster</a>
	      /
	      <a target="_blank" href="https://www.youtube.com/watch?v=RJJN45cjDrI">demo</a>
	      /
	      <a target="_blank" href="data/Neuro-Symbolic Theorem Proving with Lean.pdf">slides</a>
	      /
	      <a target="_blank" href="https://www.marktechpost.com/2024/07/30/lean-copilot-an-ai-tool-that-allows-large-language-models-llms-to-be-used-in-lean-for-proof-automation/">media</a>
              <p></p>
              <p>
                We introduce a framework for running neural network inference directly in Lean. It enables programmers to build various LLM-based proof automation tools that integrate seamlessly into the workflow of Lean users, including tools for suggesting proof steps and completing intermediate proof goals using LLMs.
              </p>
            </td>
          </tr>
		
	  <tr onmouseout="delaynet_stop()" onmouseover="delaynet_start()">
            <td style="padding:5px;width:17%;vertical-align:middle">
              <div class="one">
                <div class="two" id='delaynet_image'>
                  <img src='images/delaynet-1.png' width="150">
                </div>
                <img src='images/delaynet-2.png' width="150">
              </div>
              <script type="text/javascript">
                function delaynet_start() {
                  document.getElementById('delaynet_image').style.opacity = "1";
                }

                function delaynet_stop() {
                  document.getElementById('delaynet_image').style.opacity = "0";
                }
                delaynet_stop()
              </script>
            </td>
            <td style="padding:20px;width:83%;vertical-align:middle">
              <a target="_blank" href="https://dl.acm.org/doi/10.1145/3620665.3640395">
                <papertitle>Energy Efficient Convolutions with Temporal Arithmetic</papertitle>
              </a>
              <br />
	      <a target="_blank" href="https://www.linkedin.com/in/rhys-gretsch-462a951ab">Rhys Gretsch</a>, <strong>Peiyang Song</strong>, <a target="_blank" href="https://ireap.umd.edu/clark/staff/1448/Advait-Madhavan">Advait Madhavan</a>, <a style="text-decoration: none" target="_blank" href="https://dl.acm.org/profile/81100206077">Jeremy Lau</a>, and <a style="text-decoration: none" target="_blank" href="https://www.arch.cs.ucsb.edu/prof-sherwood">Tim Sherwood</a>
	      <br />
              <em>ACM Int'l Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</em>, 2024
              <br />
	      <a target="_blank" href="https://dl.acm.org/doi/10.1145/3620665.3640395">paper</a>
              <p></p>
              <p>
                We introduce energy-efficient convolution that improve the energy per pixel of each convolution frame by more than 2× compared to a state-of-the-art while improving the energy delay product by four orders of magnitude, by developing a new temporal arithmetic with a negative log transformation.
              </p>
            </td>
          </tr>
		
          <tr onmouseout="leandojo_stop()" onmouseover="leandojo_start()">
            <td style="padding:5px;width:17%;vertical-align:middle">
              <div class="one">
                <div class="two" id='leandojo_image'>
                  <img src='images/leandojo-1.png' width="150">
                </div>
                <img src='images/leandojo-2.png' width="150">
              </div>
              <script type="text/javascript">
                function leandojo_start() {
                  document.getElementById('leandojo_image').style.opacity = "1";
                }

                function leandojo_stop() {
                  document.getElementById('leandojo_image').style.opacity = "0";
                }
                leandojo_stop()
              </script>
            </td>
            <td style="padding:20px;width:83%;vertical-align:middle">
              <a target="_blank" href="https://leandojo.org/">
                <papertitle>LeanDojo: Theorem Proving with Retrieval-Augmented Language Models</papertitle>
              </a>
              <br />
              <a target="_blank" href="https://yangky11.github.io/">Kaiyu Yang</a>, <a target="_blank" href="https://aidanswope.com/about">Aidan Swope</a>, <a target="_blank" href="https://minimario.github.io/">Alex Gu</a>, <a target="_blank" href="https://rchalamala.github.io/">Rahul Chalamala</a>, <strong>Peiyang Song</strong>, <a target="_blank" href="https://billysx.github.io/">Shixing Yu</a>, <a target="_blank" href="https://www.linkedin.com/in/saad-godil-9728353/">Saad Godil</a>, <a target="_blank" href="https://www.linkedin.com/in/ryan-prenger-18797ba1/">Ryan Prenger</a>, and <a target="_blank" href="http://tensorlab.cms.caltech.edu/users/anima/">Anima Anandkumar</a>
              <br />
              <em>Neural Information Processing Systems (NeurIPS), Datasets and Benchmarks Track</em>, 2023, <font color="red"><strong>Oral presentation</strong></font>
	      <br />
              <a target="_blank" href="https://arxiv.org/abs/2306.15626">arXiv</a>
	      /
              <a target="_blank" href="https://leandojo.org/">project</a>
	      /
	      <a target="_blank" href="https://github.com/orgs/lean-dojo">code</a>
              /
              <a target="_blank" href="data/leandojo_poster.pdf">poster</a>
	      /
	      <a target="_blank" href="data/Neuro-Symbolic Theorem Proving with Lean.pdf">slides</a>
              /
	      <a target="_blank" href="https://www.marktechpost.com/2023/07/01/can-llms-generate-mathematical-proofs-that-can-be-rigorously-checked-meet-leandojo-an-open-source-ai-playground-with-toolkits-benchmarks-and-models-for-large-language-models-to-prove-formal-theore/">media</a>
              <p></p>
              <p>Can LLMs generate mathematical proofs that can be rigorously checked? We release LeanDojo: an open-source playground consisting of toolkits, benchmarks, and models for LLMs to prove formal theorems in the Lean proof assistant.</p>
            </td>
          </tr>	

        </tbody></table>

	<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Awards</heading>
		<ul>
		  <li>Early Research Scholarship (2023)</li>
		  <li>Caltech SURF Award (2023)</li>
		  <li>UCSB Creative Studies Honors (2022)</li>
		</ul>
            </td>
          </tr>
        </tbody></table>
	      
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br />
              <p style="text-align:right;font-size:small;">
                <a target="_blank" href="https://jonbarron.info/" style="text-align:right;font-size:small;">Site source</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>
</html>
